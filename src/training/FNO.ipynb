{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6042190e",
   "metadata": {},
   "source": [
    "# FNO Time Evolution Operator for 2D Cylinder Flow\n",
    "\n",
    "This notebook implements a Fourier Neural Operator (FNO) as a neural time integrator for 2D cylinder flow using the CFDBench dataset.\n",
    "\n",
    "**Learning Objective**: Learn the discrete Navier-Stokes flow map:\n",
    "$$u^{t+\\Delta t}(x) = G_\\theta(u^t(x), BC, \\nu, \\rho, geometry)$$\n",
    "\n",
    "The model is trained as a one-step predictor and advanced autoregressively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b375c6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: Apple Metal Performance Shaders (MPS)\n",
      "Note: PYTORCH_ENABLE_MPS_FALLBACK=1 enabled\n",
      "      FFT operations will automatically fall back to CPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# CRITICAL: Set environment variable BEFORE importing torch\n",
    "# This enables automatic CPU fallback for unsupported MPS operations (like FFT)\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Tuple, List, Dict\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path to import data loader\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from data_handle.data import get_cylinder_auto_datasets\n",
    "\n",
    "# Set device - prioritize Apple's MPS (Metal Performance Shaders) for Apple Silicon\n",
    "# With PYTORCH_ENABLE_MPS_FALLBACK=1, unsupported ops (like FFT) will automatically use CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    fft_device = torch.device('cpu')  # FFT operations must use CPU\n",
    "    print(\"Using device: Apple Metal Performance Shaders (MPS)\")\n",
    "    print(\"Note: PYTORCH_ENABLE_MPS_FALLBACK=1 enabled\")\n",
    "    print(\"      FFT operations will automatically fall back to CPU\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    fft_device = torch.device('cuda')\n",
    "    print(\"Using device: CUDA\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    fft_device = torch.device('cpu')\n",
    "    print(\"Using device: CPU\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simple train/test split function (replaces sklearn)\n",
    "def train_test_split(*arrays, test_size=0.3, random_state=None, shuffle=True):\n",
    "    \"\"\"Simple train/test split without sklearn dependency\n",
    "    Compatible with sklearn's train_test_split API\"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    n_samples = len(arrays[0])\n",
    "    n_test = int(n_samples * test_size)\n",
    "    indices = np.arange(n_samples)\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    test_indices = indices[:n_test]\n",
    "    train_indices = indices[n_test:]\n",
    "    \n",
    "    result = []\n",
    "    for array in arrays:\n",
    "        if isinstance(array, (list, tuple)):\n",
    "            train_data = [array[i] for i in train_indices]\n",
    "            test_data = [array[i] for i in test_indices]\n",
    "        else:\n",
    "            train_data = array[train_indices]\n",
    "            test_data = array[test_indices]\n",
    "        result.extend([train_data, test_data])\n",
    "    \n",
    "    return tuple(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45822566",
   "metadata": {},
   "source": [
    "## 1. Data Loading with JSON Metadata\n",
    "\n",
    "Load data from different boundary condition subsets and include JSON metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a612a42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching in subset: bc\n",
      "Data directory: /Users/thaddeusli/Desktop/PH188CFD\n",
      "==== Number of cases in different splits (Subset) ====\n",
      "train: 40, dev: 5, test: 5\n",
      "=============================================\n",
      "  [Loader]: Processing data for split 'train' from 40 case(s).\n",
      "    ⚠ case0043: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0043: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 4.3999999999999995, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0001: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0001: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 0.2, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0028: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0028: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 2.9000000000000004, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0014: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0014: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 1.5000000000000002, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0036: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0036: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 3.7, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0012: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0012: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 1.3000000000000003, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ✓ case0000: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 0.1, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0027: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0027: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 2.8000000000000003, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0047: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0047: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 4.8, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0007: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0007: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 0.8, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0033: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0033: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 3.4000000000000004, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0034: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0034: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 3.5000000000000004, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0020: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0020: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 2.1, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0049: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0049: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 5.0, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0005: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0005: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 0.6, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0038: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0038: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 3.9000000000000004, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0011: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0011: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 1.2000000000000002, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0023: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0023: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 2.4000000000000004, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0040: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0040: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 4.1, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0015: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0015: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 1.6, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0010: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0010: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 1.1, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0021: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0021: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 2.2, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0046: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0046: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 4.7, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0003: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0003: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 0.4, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0009: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0009: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 1.0, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0004: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0004: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 0.5, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0041: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0041: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 4.2, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0042: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0042: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 4.3, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0039: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0039: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 4.0, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0017: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0017: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 1.8000000000000003, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0029: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0029: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 3.0000000000000004, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0045: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0045: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 4.6, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0006: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0006: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 0.7000000000000001, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0035: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0035: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 3.6, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0018: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0018: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 1.9000000000000001, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0008: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0008: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 0.9, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0044: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0044: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 4.5, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0013: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0013: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 1.4000000000000001, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0037: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0037: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 3.8000000000000003, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0022: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0022: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 2.3000000000000003, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "  [Loader]: Using data shape: (620, 64, 64) (time_steps=620, spatial=64x64)\n",
      "  [Loader]: Processing data for split 'dev' from 5 case(s).\n",
      "    ⚠ case0030: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0030: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 3.1, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0019: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0019: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 2.0, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0025: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0025: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 2.6, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0031: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0031: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 3.2, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0032: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0032: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 3.3000000000000003, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "  [Loader]: Using data shape: (620, 64, 64) (time_steps=620, spatial=64x64)\n",
      "  [Loader]: Processing data for split 'test' from 5 case(s).\n",
      "    ⚠ case0016: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0016: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 1.7000000000000002, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0002: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0002: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 0.30000000000000004, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0026: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0026: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 2.7, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0048: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0048: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 4.9, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "    ⚠ case0024: Truncated from 1000 to 620 timesteps\n",
      "    ✓ case0024: u.npy shape (620, 64, 64), v.npy shape (620, 64, 64), params: {'vel_in': 2.5000000000000004, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n",
      "  [Loader]: Using data shape: (620, 64, 64) (time_steps=620, spatial=64x64)\n",
      "Total cases loaded: 45\n",
      "Data shape: (620, 64, 64)\n",
      "Sample parameters: {'vel_in': 4.3999999999999995, 'density': 10, 'viscosity': 0.001, 'radius': 0.01, 'x_min': -0.06, 'x_max': 0.16, 'y_min': -0.06, 'y_max': 0.06}\n"
     ]
    }
   ],
   "source": [
    "# Load data from BC subset (boundary conditions)\n",
    "DATA_DIR = Path(\"../../../\")\n",
    "bc_train_data, bc_dev_data, bc_test_data = get_cylinder_auto_datasets(\n",
    "    data_dir=DATA_DIR,\n",
    "    subset_name=\"bc\",\n",
    "    norm_props=True,\n",
    "    norm_bc=True,\n",
    "    case_fraction=1.0,  # Use all cases\n",
    "    load_splits=[\"train\", \"dev\", \"test\"]\n",
    ")\n",
    "\n",
    "# Combine train and dev for 70/30 split\n",
    "all_cases = []\n",
    "all_u_data = []\n",
    "all_v_data = []\n",
    "all_params = []\n",
    "\n",
    "if bc_train_data:\n",
    "    all_cases.extend(bc_train_data.case_names)\n",
    "    all_u_data.extend(bc_train_data.u_data)\n",
    "    all_v_data.extend(bc_train_data.v_data)\n",
    "    all_params.extend(bc_train_data.case_params)\n",
    "\n",
    "if bc_dev_data:\n",
    "    all_cases.extend(bc_dev_data.case_names)\n",
    "    all_u_data.extend(bc_dev_data.u_data)\n",
    "    all_v_data.extend(bc_dev_data.v_data)\n",
    "    all_params.extend(bc_dev_data.case_params)\n",
    "\n",
    "print(f\"Total cases loaded: {len(all_cases)}\")\n",
    "print(f\"Data shape: {all_u_data[0].shape}\")\n",
    "print(f\"Sample parameters: {all_params[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850fc834",
   "metadata": {},
   "source": [
    "## 2. Geometry Mask Generation\n",
    "\n",
    "Create binary mask for cylinder obstacle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88cd5449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask shape: (64, 64)\n",
      "Mask values: min=0.0, max=1.0, mean=0.996826171875\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_geometry_mask(params: Dict, grid_size: Tuple[int, int] = (64, 64)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create binary mask for cylinder obstacle.\n",
    "    mask(x) = 1 inside cylinder, 0 elsewhere\n",
    "    \"\"\"\n",
    "    H, W = grid_size\n",
    "    mask = np.ones((H, W), dtype=np.float32)\n",
    "    \n",
    "    # Extract geometry parameters\n",
    "    radius = params['radius']\n",
    "    x_min = params['x_min']\n",
    "    x_max = params['x_max']\n",
    "    y_min = params['y_min']\n",
    "    y_max = params['y_max']\n",
    "    \n",
    "    # Calculate cylinder center (assuming it's centered in y, positioned at x=0 relative to domain)\n",
    "    # Domain spans from x_min to x_max, y_min to y_max\n",
    "    domain_width = x_max - x_min\n",
    "    domain_height = y_max - y_min\n",
    "    \n",
    "    # Cylinder center is at x=0 (relative to x_min), y=0 (center of domain)\n",
    "    center_x_rel = 0.0 - x_min  # Relative to domain start\n",
    "    center_y_rel = (y_max + y_min) / 2 - y_min  # Center of domain\n",
    "    \n",
    "    # Convert to grid coordinates\n",
    "    center_x_grid = int((center_x_rel / domain_width) * W)\n",
    "    center_y_grid = int((center_y_rel / domain_height) * H)\n",
    "    radius_grid = int((radius / domain_width) * W)\n",
    "    \n",
    "    # Create coordinate grids\n",
    "    y_coords, x_coords = np.ogrid[:H, :W]\n",
    "    \n",
    "    # Distance from center\n",
    "    dist_from_center = np.sqrt((x_coords - center_x_grid)**2 + (y_coords - center_y_grid)**2)\n",
    "    \n",
    "    # Set mask to 0 inside cylinder\n",
    "    mask[dist_from_center <= radius_grid] = 0.0\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Test mask generation\n",
    "test_mask = create_geometry_mask(all_params[0])\n",
    "print(f\"Mask shape: {test_mask.shape}\")\n",
    "print(f\"Mask values: min={test_mask.min()}, max={test_mask.max()}, mean={test_mask.mean()}\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(test_mask, cmap='gray', origin='lower')\n",
    "plt.title('Geometry Mask (0=cylinder, 1=fluid)')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4174d4a6",
   "metadata": {},
   "source": [
    "## 3. FNO Architecture Implementation\n",
    "\n",
    "Implement Fourier Neural Operator according to roadmap specifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adcdd69",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS device does not support bmm for non-float inputs",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 146\u001b[39m\n\u001b[32m    144\u001b[39m test_model = FNO(in_channels=\u001b[32m11\u001b[39m, out_channels=\u001b[32m2\u001b[39m, width=\u001b[32m64\u001b[39m, modes1=\u001b[32m16\u001b[39m, modes2=\u001b[32m16\u001b[39m, n_layers=\u001b[32m4\u001b[39m).to(device)\n\u001b[32m    145\u001b[39m test_input = torch.randn(\u001b[32m2\u001b[39m, \u001b[32m64\u001b[39m, \u001b[32m64\u001b[39m, \u001b[32m11\u001b[39m).to(device)\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m test_output = test_model(test_input)\n\u001b[32m    147\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_input.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    148\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOutput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_output.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/capstone188/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/capstone188/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 128\u001b[39m, in \u001b[36mFNO.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# Fourier layers\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fno_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fno_blocks:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     x = fno_block(x)\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# Projection\u001b[39;00m\n\u001b[32m    131\u001b[39m x = x.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# [B, H, W, width]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/capstone188/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/capstone188/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mFNOBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.norm(\u001b[38;5;28mself\u001b[39m.activation(\u001b[38;5;28mself\u001b[39m.conv(x) + \u001b[38;5;28mself\u001b[39m.w(x)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/capstone188/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/capstone188/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mSpectralConv2d.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Multiply relevant Fourier modes (weights are on CPU)\u001b[39;00m\n\u001b[32m     45\u001b[39m out_ft = torch.zeros(batchsize, \u001b[38;5;28mself\u001b[39m.out_channels, x.size(-\u001b[32m2\u001b[39m), x.size(-\u001b[32m1\u001b[39m)//\u001b[32m2\u001b[39m + \u001b[32m1\u001b[39m, \n\u001b[32m     46\u001b[39m                     dtype=torch.cfloat, device=\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     48\u001b[39m out_ft[:, :, :\u001b[38;5;28mself\u001b[39m.modes1, :\u001b[38;5;28mself\u001b[39m.modes2] = \\\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     torch.einsum(\u001b[33m\"\u001b[39m\u001b[33mbixy,ioxy->boxy\u001b[39m\u001b[33m\"\u001b[39m, x_ft[:, :, :\u001b[38;5;28mself\u001b[39m.modes1, :\u001b[38;5;28mself\u001b[39m.modes2], \u001b[38;5;28mself\u001b[39m.weights1)\n\u001b[32m     50\u001b[39m out_ft[:, :, -\u001b[38;5;28mself\u001b[39m.modes1:, :\u001b[38;5;28mself\u001b[39m.modes2] = \\\n\u001b[32m     51\u001b[39m     torch.einsum(\u001b[33m\"\u001b[39m\u001b[33mbixy,ioxy->boxy\u001b[39m\u001b[33m\"\u001b[39m, x_ft[:, :, -\u001b[38;5;28mself\u001b[39m.modes1:, :\u001b[38;5;28mself\u001b[39m.modes2], \u001b[38;5;28mself\u001b[39m.weights2)\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Return to physical space on CPU\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/capstone188/lib/python3.12/site-packages/torch/functional.py:380\u001b[39m, in \u001b[36meinsum\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, *_operands)\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) <= \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum.enabled:\n\u001b[32m    378\u001b[39m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[32m    379\u001b[39m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF.einsum(equation, operands)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    382\u001b[39m path = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum.is_available():\n",
      "\u001b[31mRuntimeError\u001b[39m: MPS device does not support bmm for non-float inputs"
     ]
    }
   ],
   "source": [
    "class SpectralConv2d(nn.Module):\n",
    "    \"\"\"2D Spectral Convolution Layer\n",
    "    \n",
    "    Hybrid MPS/CPU implementation: Uses MPS for most ops, CPU for FFT.\n",
    "    Falls back to manual CPU transfer if PYTORCH_ENABLE_MPS_FALLBACK doesn't work.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, modes1: int, modes2: int):\n",
    "        super(SpectralConv2d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1  # Number of Fourier modes to multiply\n",
    "        self.modes2 = modes2\n",
    "        \n",
    "        # Complex weights - keep on CPU (MPS doesn't support complex operations)\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(\n",
    "            self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, \n",
    "                                   dtype=torch.cfloat, device=torch.device('cpu'))\n",
    "        )\n",
    "        self.weights2 = nn.Parameter(\n",
    "            self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, \n",
    "                                   dtype=torch.cfloat, device=torch.device('cpu'))\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass with hybrid MPS/CPU:\n",
    "        All complex operations (FFT, einsum) happen on CPU\n",
    "        \"\"\"\n",
    "        batchsize = x.shape[0]\n",
    "        original_device = x.device\n",
    "        \n",
    "        # CRITICAL: Move to CPU for ALL complex operations\n",
    "        # MPS doesn't support FFT or complex einsum operations\n",
    "        if original_device.type == 'mps':\n",
    "            x_cpu = x.to('cpu', non_blocking=False)\n",
    "        else:\n",
    "            x_cpu = x\n",
    "        \n",
    "        # Compute Fourier coefficients on CPU\n",
    "        x_ft = torch.fft.rfft2(x_cpu)\n",
    "        \n",
    "        # Ensure weights are on CPU (they should be, but double-check)\n",
    "        weights1_cpu = self.weights1.to('cpu')\n",
    "        weights2_cpu = self.weights2.to('cpu')\n",
    "        \n",
    "        # Multiply relevant Fourier modes (ALL on CPU - einsum with complex needs CPU)\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels, x.size(-2), x.size(-1)//2 + 1, \n",
    "                            dtype=torch.cfloat, device='cpu')\n",
    "        \n",
    "        # Einsum operations must be on CPU (MPS doesn't support complex einsum)\n",
    "        # x_ft is already on CPU, weights are on CPU, so einsum will be on CPU\n",
    "        out_ft[:, :, :self.modes1, :self.modes2] = \\\n",
    "            torch.einsum(\"bixy,ioxy->boxy\", \n",
    "                        x_ft[:, :, :self.modes1, :self.modes2], \n",
    "                        weights1_cpu)\n",
    "        out_ft[:, :, -self.modes1:, :self.modes2] = \\\n",
    "            torch.einsum(\"bixy,ioxy->boxy\", \n",
    "                        x_ft[:, :, -self.modes1:, :self.modes2], \n",
    "                        weights2_cpu)\n",
    "        \n",
    "        # Return to physical space on CPU\n",
    "        x_out = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))\n",
    "        \n",
    "        # Move back to original device (MPS) for subsequent operations\n",
    "        if original_device.type == 'mps':\n",
    "            x_out = x_out.to(original_device, non_blocking=True)\n",
    "        \n",
    "        return x_out\n",
    "\n",
    "\n",
    "class FNOBlock(nn.Module):\n",
    "    \"\"\"FNO Block: Spectral Convolution + Pointwise Convolution\"\"\"\n",
    "    def __init__(self, width: int, modes1: int, modes2: int):\n",
    "        super(FNOBlock, self).__init__()\n",
    "        self.conv = SpectralConv2d(width, width, modes1, modes2)\n",
    "        self.w = nn.Conv2d(width, width, 1)  # Pointwise convolution\n",
    "        self.activation = nn.GELU()\n",
    "        self.norm = nn.InstanceNorm2d(width)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.norm(self.activation(self.conv(x) + self.w(x)))\n",
    "\n",
    "\n",
    "class FNO(nn.Module):\n",
    "    \"\"\"\n",
    "    Fourier Neural Operator for 2D Navier-Stokes time evolution\n",
    "    \n",
    "    Input channels: [u_x^t, u_y^t, u_B, ρ, μ, d, x1, x2, y1, y2, mask] = 11 channels\n",
    "    Output channels: [u_x^{t+1}, u_y^{t+1}] = 2 channels\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 11,\n",
    "        out_channels: int = 2,\n",
    "        width: int = 64,\n",
    "        modes1: int = 16,\n",
    "        modes2: int = 16,\n",
    "        n_layers: int = 4,\n",
    "        padding: int = 20  # Zero padding for boundary conditions\n",
    "    ):\n",
    "        super(FNO, self).__init__()\n",
    "        self.padding = padding\n",
    "        \n",
    "        # Lifting layer: project to higher dimension\n",
    "        self.fc0 = nn.Linear(in_channels, width)\n",
    "        \n",
    "        # Fourier layers (FFT device is handled internally in SpectralConv2d)\n",
    "        self.fno_blocks = nn.ModuleList([\n",
    "            FNOBlock(width, modes1, modes2) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Projection layer: back to output dimension\n",
    "        self.fc1 = nn.Linear(width, 128)\n",
    "        self.fc2 = nn.Linear(128, out_channels)\n",
    "        self.activation = nn.GELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, H, W, in_channels]\n",
    "        Returns: [B, H, W, out_channels]\n",
    "        \"\"\"\n",
    "        # Zero padding for boundary conditions (Option A from roadmap)\n",
    "        if self.padding > 0:\n",
    "            x = torch.nn.functional.pad(x, (0, 0, self.padding, self.padding, self.padding, self.padding), mode='constant', value=0)\n",
    "        \n",
    "        # Convert to [B, in_channels, H, W] for conv layers\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Lifting\n",
    "        x = x.permute(0, 2, 3, 1)  # [B, H, W, in_channels]\n",
    "        x = self.fc0(x)  # [B, H, W, width]\n",
    "        x = x.permute(0, 3, 1, 2)  # [B, width, H, W]\n",
    "        \n",
    "        # Fourier layers\n",
    "        for fno_block in self.fno_blocks:\n",
    "            x = fno_block(x)\n",
    "        \n",
    "        # Projection\n",
    "        x = x.permute(0, 2, 3, 1)  # [B, H, W, width]\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)  # [B, H, W, out_channels]\n",
    "        \n",
    "        # Crop back to original size\n",
    "        if self.padding > 0:\n",
    "            x = x[:, self.padding:-self.padding, self.padding:-self.padding, :]\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test FNO architecture\n",
    "# Hybrid: Model on MPS, FFT on CPU automatically\n",
    "test_model = FNO(in_channels=11, out_channels=2, width=64, modes1=16, modes2=16, n_layers=4).to(device)\n",
    "test_input = torch.randn(2, 64, 64, 11).to(device)\n",
    "test_output = test_model(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in test_model.parameters()):,}\")\n",
    "print(f\"Model device: {next(test_model.parameters()).device}\")\n",
    "print(f\"Output device: {test_output.device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a683e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e30fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset (lazy-loading)...\n",
      "Dataset created with 27675 samples from 45 cases\n",
      "Total samples: 27675\n",
      "Sample input shape: torch.Size([64, 64, 11])\n",
      "Sample target shape: torch.Size([5, 64, 64, 2])\n"
     ]
    }
   ],
   "source": [
    "def prepare_input_channels(u_t: np.ndarray, v_t: np.ndarray, params: Dict, mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create input tensor with 11 channels:\n",
    "    [u_x^t, u_y^t, u_B, ρ, μ, d, x1, x2, y1, y2, mask]\n",
    "    \"\"\"\n",
    "    H, W = u_t.shape[-2], u_t.shape[-1]\n",
    "    \n",
    "    # Broadcast scalar parameters to spatial dimensions\n",
    "    u_B = np.full((H, W), params['vel_in'], dtype=np.float32)\n",
    "    rho = np.full((H, W), params['density'], dtype=np.float32)\n",
    "    mu = np.full((H, W), params['viscosity'], dtype=np.float32)\n",
    "    d = np.full((H, W), params['radius'] * 2, dtype=np.float32)  # diameter\n",
    "    x1 = np.full((H, W), params['x_min'], dtype=np.float32)\n",
    "    x2 = np.full((H, W), params['x_max'], dtype=np.float32)\n",
    "    y1 = np.full((H, W), params['y_min'], dtype=np.float32)\n",
    "    y2 = np.full((H, W), params['y_max'], dtype=np.float32)\n",
    "    \n",
    "    # Stack channels: [u_x, u_y, u_B, ρ, μ, d, x1, x2, y1, y2, mask]\n",
    "    input_channels = np.stack([\n",
    "        u_t, v_t, u_B, rho, mu, d, x1, x2, y1, y2, mask\n",
    "    ], axis=-1)  # Shape: (H, W, 11)\n",
    "    \n",
    "    return input_channels\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CylinderFlowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Lazy-loading dataset that generates samples on-the-fly instead of pre-loading everything.\n",
    "    This prevents memory issues with large datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, u_data_list: List[np.ndarray], v_data_list: List[np.ndarray],\n",
    "                 params_list: List[Dict], masks_list: List[np.ndarray], \n",
    "                 rollout_steps: int = 5):\n",
    "        self.u_data_list = u_data_list\n",
    "        self.v_data_list = v_data_list\n",
    "        self.params_list = params_list\n",
    "        self.masks_list = masks_list\n",
    "        self.rollout_steps = rollout_steps\n",
    "        \n",
    "        # Pre-compute indices for efficient access\n",
    "        self.indices = []\n",
    "        for case_idx, (u_data, v_data) in enumerate(zip(u_data_list, v_data_list)):\n",
    "            T = u_data.shape[0]\n",
    "            for t in range(T - rollout_steps):\n",
    "                self.indices.append((case_idx, t))\n",
    "        \n",
    "        print(f\"Dataset created with {len(self.indices)} samples from {len(u_data_list)} cases\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        case_idx, t = self.indices[idx]\n",
    "        \n",
    "        # Get data for this case\n",
    "        u_data = self.u_data_list[case_idx]\n",
    "        v_data = self.v_data_list[case_idx]\n",
    "        params = self.params_list[case_idx]\n",
    "        mask = self.masks_list[case_idx]\n",
    "        \n",
    "        # Current state\n",
    "        u_t = u_data[t]\n",
    "        v_t = v_data[t]\n",
    "        \n",
    "        # Prepare input\n",
    "        input_channels = prepare_input_channels(u_t, v_t, params, mask)\n",
    "        \n",
    "        # Target: future states (K steps ahead)\n",
    "        target_u = u_data[t+1:t+self.rollout_steps+1]  # Shape: (K, H, W)\n",
    "        target_v = v_data[t+1:t+self.rollout_steps+1]  # Shape: (K, H, W)\n",
    "        target = np.stack([target_u, target_v], axis=-1)  # Shape: (K, H, W, 2)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        input_tensor = torch.FloatTensor(input_channels)\n",
    "        target_tensor = torch.FloatTensor(target)\n",
    "        \n",
    "        return input_tensor, target_tensor\n",
    "\n",
    "# Create masks for all cases\n",
    "masks_list = [create_geometry_mask(params) for params in all_params]\n",
    "\n",
    "# Create lazy-loading dataset (doesn't load all data into memory)\n",
    "print(\"Creating dataset (lazy-loading)...\")\n",
    "full_dataset = CylinderFlowDataset(all_u_data, all_v_data, all_params, masks_list, rollout_steps=5)\n",
    "print(f\"Total samples: {len(full_dataset)}\")\n",
    "\n",
    "# Get a small sample to compute statistics (for normalization if needed)\n",
    "sample_input, sample_target = full_dataset[0]\n",
    "print(f\"Sample input shape: {sample_input.shape}\")\n",
    "print(f\"Sample target shape: {sample_target.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cdb881",
   "metadata": {},
   "source": [
    "## 5. Train/Test Split (70% Training, 30% Evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3319d2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 19372\n",
      "Evaluation samples: 8303\n",
      "Note: Data is loaded on-the-fly in batches, not all at once\n"
     ]
    }
   ],
   "source": [
    "# 70/30 split using indices (more memory efficient)\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Create indices for train/test split\n",
    "n_samples = len(full_dataset)\n",
    "n_train = int(n_samples * 0.7)\n",
    "indices = np.arange(n_samples)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_indices = indices[:n_train]\n",
    "eval_indices = indices[n_train:]\n",
    "\n",
    "# Create subset datasets (still lazy-loading)\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "eval_dataset = Subset(full_dataset, eval_indices)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Evaluation samples: {len(eval_dataset)}\")\n",
    "print(\"Note: Data is loaded on-the-fly in batches, not all at once\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f092f5ef",
   "metadata": {},
   "source": [
    "## 6. Training Code Block\n",
    "\n",
    "Train FNO with autoregressive K-step rollout loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61088e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: mps\n",
      "Note: FFT operations will use CPU (manual transfer for reliability)\n",
      "Ensured fno_blocks.0.conv.weights1 is on CPU (complex dtype)\n",
      "Ensured fno_blocks.0.conv.weights2 is on CPU (complex dtype)\n",
      "Ensured fno_blocks.1.conv.weights1 is on CPU (complex dtype)\n",
      "Ensured fno_blocks.1.conv.weights2 is on CPU (complex dtype)\n",
      "Ensured fno_blocks.2.conv.weights1 is on CPU (complex dtype)\n",
      "Ensured fno_blocks.2.conv.weights2 is on CPU (complex dtype)\n",
      "Ensured fno_blocks.3.conv.weights1 is on CPU (complex dtype)\n",
      "Ensured fno_blocks.3.conv.weights2 is on CPU (complex dtype)\n",
      "Using rollout_steps=3 (reduced for faster training)\n",
      "Batch size: 4\n",
      "Batches per epoch: 4843\n",
      "Estimated samples per epoch: 19372\n",
      "Starting training...\n",
      "Total epochs: 50, Batches per epoch: 4843\n",
      "Total batches: 242150\n",
      "------------------------------------------------------------\n",
      "Testing first batch...\n",
      "First batch loaded: inputs shape=torch.Size([4, 64, 64, 11]), targets shape=torch.Size([4, 5, 64, 64, 2])\n",
      "Starting training loop...\n",
      "------------------------------------------------------------\n",
      "Processing first batch: inputs device=cpu, shape=torch.Size([4, 64, 64, 11])\n",
      "Epoch [1/50], Batch [10/4843], Loss: 18.651365, Time: 1.15s\n",
      "Epoch [1/50], Batch [20/4843], Loss: 0.962664, Time: 1.39s\n",
      "Epoch [1/50], Batch [30/4843], Loss: 3.464312, Time: 1.13s\n",
      "Epoch [1/50], Batch [40/4843], Loss: 3.080772, Time: 1.14s\n",
      "Epoch [1/50], Batch [50/4843], Loss: 2.140265, Time: 1.26s\n",
      "Epoch [1/50], Batch [60/4843], Loss: 2.011809, Time: 1.17s\n",
      "Epoch [1/50], Batch [70/4843], Loss: 0.489790, Time: 1.24s\n",
      "Epoch [1/50], Batch [80/4843], Loss: 2.165810, Time: 1.15s\n",
      "Epoch [1/50], Batch [90/4843], Loss: 0.227505, Time: 1.13s\n",
      "Epoch [1/50], Batch [100/4843], Loss: 1.492864, Time: 1.56s\n",
      "Epoch [1/50], Batch [110/4843], Loss: 0.484451, Time: 1.18s\n",
      "Epoch [1/50], Batch [120/4843], Loss: 1.541263, Time: 1.35s\n",
      "Epoch [1/50], Batch [130/4843], Loss: 3.099172, Time: 1.22s\n",
      "Epoch [1/50], Batch [140/4843], Loss: 2.810690, Time: 1.31s\n",
      "Epoch [1/50], Batch [150/4843], Loss: 0.642444, Time: 1.32s\n",
      "Epoch [1/50], Batch [160/4843], Loss: 0.419365, Time: 1.15s\n",
      "Epoch [1/50], Batch [170/4843], Loss: 0.509201, Time: 1.19s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 113\u001b[39m\n\u001b[32m    110\u001b[39m loss = total_loss / rollout_steps\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m loss.backward()\n\u001b[32m    114\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m    115\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/capstone188/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    513\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    514\u001b[39m         Tensor.backward,\n\u001b[32m    515\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    520\u001b[39m         inputs=inputs,\n\u001b[32m    521\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m torch.autograd.backward(\n\u001b[32m    523\u001b[39m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs=inputs\n\u001b[32m    524\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/capstone188/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    261\u001b[39m     retain_graph = create_graph\n\u001b[32m    263\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m Variable._execution_engine.run_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    267\u001b[39m     tensors,\n\u001b[32m    268\u001b[39m     grad_tensors_,\n\u001b[32m    269\u001b[39m     retain_graph,\n\u001b[32m    270\u001b[39m     create_graph,\n\u001b[32m    271\u001b[39m     inputs,\n\u001b[32m    272\u001b[39m     allow_unreachable=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    273\u001b[39m     accumulate_grad=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    274\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "# Hybrid approach: Model on MPS, FFT operations manually handled on CPU\n",
    "model_device = device  # Use MPS if available\n",
    "print(f\"Model device: {model_device}\")\n",
    "print(\"Note: FFT operations will use CPU (manual transfer for reliability)\")\n",
    "\n",
    "# Create model\n",
    "model = FNO(in_channels=11, out_channels=2, width=64, modes1=16, modes2=16, n_layers=4, padding=20).to(model_device)\n",
    "\n",
    "# Ensure complex weights stay on CPU (they're created on CPU, but .to() might move them)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.dtype == torch.cfloat:\n",
    "        param.data = param.data.to('cpu')\n",
    "        print(f\"Ensured {name} is on CPU (complex dtype)\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 4  # Reduced batch size to speed up per-batch processing\n",
    "n_epochs = 50\n",
    "rollout_steps = 3  # K-step rollout (reduced from 5 for faster training - can increase later)\n",
    "print(f\"Using rollout_steps={rollout_steps} (reduced for faster training)\")\n",
    "\n",
    "# Create data loaders (with lazy loading)\n",
    "# Use pin_memory=True for faster CPU->GPU transfers\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0,\n",
    "    pin_memory=True if model_device.type != 'cpu' else False\n",
    ")\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=0,\n",
    "    pin_memory=True if model_device.type != 'cpu' else False\n",
    ")\n",
    "\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Estimated samples per epoch: {batch_size * len(train_loader)}\")\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "import time\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Total epochs: {n_epochs}, Batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Total batches: {n_epochs * len(train_loader)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Test first batch to ensure everything works\n",
    "print(\"Testing first batch...\")\n",
    "first_batch = next(iter(train_loader))\n",
    "print(f\"First batch loaded: inputs shape={first_batch[0].shape}, targets shape={first_batch[1].shape}\")\n",
    "print(\"Starting training loop...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (batch_inputs, batch_targets) in enumerate(train_loader):\n",
    "        # Print first batch to confirm we're processing\n",
    "        if batch_idx == 0 and epoch == 0:\n",
    "            print(f\"Processing first batch: inputs device={batch_inputs.device}, shape={batch_inputs.shape}\")\n",
    "        batch_start_time = time.time()\n",
    "        \n",
    "        # Move batch to model device (MPS/CPU)\n",
    "        batch_inputs = batch_inputs.to(model_device, non_blocking=True)\n",
    "        batch_targets = batch_targets.to(model_device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Autoregressive rollout loss\n",
    "        current_state = batch_inputs  # [B, H, W, 11]\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        # Predict K steps ahead autoregressively\n",
    "        for k in range(rollout_steps):\n",
    "            # Predict next step\n",
    "            pred_next = model(current_state)  # [B, H, W, 2]\n",
    "            \n",
    "            # Get target for this step\n",
    "            target_k = batch_targets[:, k, :, :, :]  # [B, H, W, 2]\n",
    "            \n",
    "            # Compute loss (L2 velocity error with energy weighting)\n",
    "            velocity_error = torch.mean((pred_next - target_k) ** 2)\n",
    "            \n",
    "            # Energy-weighted loss (kinetic energy)\n",
    "            u_pred, v_pred = pred_next[:, :, :, 0], pred_next[:, :, :, 1]\n",
    "            u_target, v_target = target_k[:, :, :, 0], target_k[:, :, :, 1]\n",
    "            kinetic_energy_pred = 0.5 * (u_pred**2 + v_pred**2)\n",
    "            kinetic_energy_target = 0.5 * (u_target**2 + v_target**2)\n",
    "            energy_error = torch.mean((kinetic_energy_pred - kinetic_energy_target) ** 2)\n",
    "            \n",
    "            # Combined loss\n",
    "            step_loss = velocity_error + 0.1 * energy_error\n",
    "            total_loss += step_loss\n",
    "            \n",
    "            # Update current state for next autoregressive step (in-place update to avoid clone)\n",
    "            current_state[:, :, :, 0] = pred_next[:, :, :, 0]  # u_x\n",
    "            current_state[:, :, :, 1] = pred_next[:, :, :, 1]  # u_y\n",
    "        \n",
    "        # Average loss over rollout steps\n",
    "        loss = total_loss / rollout_steps\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Print progress every 10 batches\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            batch_time = time.time() - batch_start_time\n",
    "            print(f\"Epoch [{epoch+1}/{n_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], \"\n",
    "                  f\"Loss: {loss.item():.6f}, Time: {batch_time:.2f}s\")\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    scheduler.step(avg_loss)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}] completed - Avg Loss: {avg_loss:.6f}, \"\n",
    "          f\"LR: {optimizer.param_groups[0]['lr']:.6f}, Time: {epoch_time:.2f}s\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1d882f",
   "metadata": {},
   "source": [
    "## 7. Evaluation Code Block\n",
    "\n",
    "Evaluate model on test set with long-horizon stability metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb35fe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "def compute_metrics(pred: torch.Tensor, target: torch.Tensor) -> Dict[str, float]:\n",
    "    \"\"\"Compute evaluation metrics\"\"\"\n",
    "    # L2 velocity error\n",
    "    l2_error = torch.mean((pred - target) ** 2).item()\n",
    "    \n",
    "    # Kinetic energy drift\n",
    "    u_pred, v_pred = pred[:, :, :, 0], pred[:, :, :, 1]\n",
    "    u_target, v_target = target[:, :, :, 0], target[:, :, :, 1]\n",
    "    \n",
    "    ke_pred = 0.5 * (u_pred**2 + v_pred**2)\n",
    "    ke_target = 0.5 * (u_target**2 + v_target**2)\n",
    "    ke_drift = torch.mean(torch.abs(ke_pred - ke_target)).item()\n",
    "    \n",
    "    # Relative error\n",
    "    relative_error = torch.mean(torch.abs(pred - target) / (torch.abs(target) + 1e-6)).item()\n",
    "    \n",
    "    return {\n",
    "        'l2_error': l2_error,\n",
    "        'ke_drift': ke_drift,\n",
    "        'relative_error': relative_error\n",
    "    }\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "eval_losses = []\n",
    "all_metrics = {'l2_error': [], 'ke_drift': [], 'relative_error': []}\n",
    "\n",
    "print(\"Starting evaluation...\")\n",
    "with torch.no_grad():\n",
    "    for batch_inputs, batch_targets in eval_loader:\n",
    "        # Move batch to model device (CPU for FNO)\n",
    "        batch_inputs = batch_inputs.to(model_device)\n",
    "        batch_targets = batch_targets.to(model_device)\n",
    "        current_state = batch_inputs\n",
    "        batch_metrics = {'l2_error': 0.0, 'ke_drift': 0.0, 'relative_error': 0.0}\n",
    "        \n",
    "        # Autoregressive prediction for K steps\n",
    "        for k in range(rollout_steps):\n",
    "            pred_next = model(current_state)\n",
    "            target_k = batch_targets[:, k, :, :, :]\n",
    "            \n",
    "            # Compute metrics\n",
    "            metrics = compute_metrics(pred_next, target_k)\n",
    "            for key in batch_metrics:\n",
    "                batch_metrics[key] += metrics[key]\n",
    "            \n",
    "            # Update state for next step\n",
    "            current_state = current_state.clone()\n",
    "            current_state[:, :, :, 0] = pred_next[:, :, :, 0]\n",
    "            current_state[:, :, :, 1] = pred_next[:, :, :, 1]\n",
    "        \n",
    "        # Average metrics over rollout steps\n",
    "        for key in batch_metrics:\n",
    "            batch_metrics[key] /= rollout_steps\n",
    "            all_metrics[key].append(batch_metrics[key])\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = batch_metrics['l2_error']\n",
    "        eval_losses.append(loss)\n",
    "\n",
    "avg_eval_loss = np.mean(eval_losses)\n",
    "print(f\"\\nEvaluation Results:\")\n",
    "print(f\"Average L2 Error: {avg_eval_loss:.6f}\")\n",
    "print(f\"Average Kinetic Energy Drift: {np.mean(all_metrics['ke_drift']):.6f}\")\n",
    "print(f\"Average Relative Error: {np.mean(all_metrics['relative_error']):.6f}\")\n",
    "\n",
    "# Plot evaluation metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].hist(all_metrics['l2_error'], bins=50)\n",
    "axes[0].set_xlabel('L2 Error')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('L2 Velocity Error Distribution')\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].hist(all_metrics['ke_drift'], bins=50)\n",
    "axes[1].set_xlabel('Kinetic Energy Drift')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Kinetic Energy Drift Distribution')\n",
    "axes[1].grid(True)\n",
    "\n",
    "axes[2].hist(all_metrics['relative_error'], bins=50)\n",
    "axes[2].set_xlabel('Relative Error')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].set_title('Relative Error Distribution')\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a02d31",
   "metadata": {},
   "source": [
    "## 8. Visualization: Predicted vs Ground Truth\n",
    "\n",
    "Visualize predictions for sample cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633ea2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get a sample from evaluation set\n",
    "    sample_input, sample_target = eval_dataset[0]\n",
    "    sample_input = sample_input.unsqueeze(0).to(model_device)  # [1, H, W, 11]\n",
    "    sample_target = sample_target.unsqueeze(0).to(model_device)  # [1, K, H, W, 2]\n",
    "    \n",
    "    # Predict autoregressively\n",
    "    current_state = sample_input\n",
    "    predictions = []\n",
    "    \n",
    "    for k in range(rollout_steps):\n",
    "        pred_next = model(current_state)\n",
    "        predictions.append(pred_next[0].cpu().numpy())  # [H, W, 2]\n",
    "        \n",
    "        # Update state\n",
    "        current_state = current_state.clone()\n",
    "        current_state[0, :, :, 0] = pred_next[0, :, :, 0]\n",
    "        current_state[0, :, :, 1] = pred_next[0, :, :, 1]\n",
    "    \n",
    "    predictions = np.array(predictions)  # [K, H, W, 2]\n",
    "    targets = sample_target[0].cpu().numpy()  # [K, H, W, 2]\n",
    "    \n",
    "    # Plot first and last timestep\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    # Timestep 0\n",
    "    axes[0, 0].imshow(predictions[0, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "    axes[0, 0].set_title('Predicted u (t+1)')\n",
    "    axes[0, 0].set_xlabel('X')\n",
    "    axes[0, 0].set_ylabel('Y')\n",
    "    \n",
    "    axes[0, 1].imshow(targets[0, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "    axes[0, 1].set_title('Ground Truth u (t+1)')\n",
    "    \n",
    "    axes[0, 2].imshow(predictions[0, :, :, 1], cmap='RdBu_r', origin='lower')\n",
    "    axes[0, 2].set_title('Predicted v (t+1)')\n",
    "    \n",
    "    axes[0, 3].imshow(targets[0, :, :, 1], cmap='RdBu_r', origin='lower')\n",
    "    axes[0, 3].set_title('Ground Truth v (t+1)')\n",
    "    \n",
    "    # Last timestep\n",
    "    last_idx = rollout_steps - 1\n",
    "    axes[1, 0].imshow(predictions[last_idx, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "    axes[1, 0].set_title(f'Predicted u (t+{rollout_steps})')\n",
    "    \n",
    "    axes[1, 1].imshow(targets[last_idx, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "    axes[1, 1].set_title(f'Ground Truth u (t+{rollout_steps})')\n",
    "    \n",
    "    axes[1, 2].imshow(predictions[last_idx, :, :, 1], cmap='RdBu_r', origin='lower')\n",
    "    axes[1, 2].set_title(f'Predicted v (t+{rollout_steps})')\n",
    "    \n",
    "    axes[1, 3].imshow(targets[last_idx, :, :, 1], cmap='RdBu_r', origin='lower')\n",
    "    axes[1, 3].set_title(f'Ground Truth v (t+{rollout_steps})')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute error maps\n",
    "    error_u = np.abs(predictions - targets)[:, :, :, 0]\n",
    "    error_v = np.abs(predictions - targets)[:, :, :, 1]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    im1 = axes[0].imshow(error_u[-1], cmap='hot', origin='lower')\n",
    "    axes[0].set_title(f'Error in u (t+{rollout_steps})')\n",
    "    plt.colorbar(im1, ax=axes[0])\n",
    "    \n",
    "    im2 = axes[1].imshow(error_v[-1], cmap='hot', origin='lower')\n",
    "    axes[1].set_title(f'Error in v (t+{rollout_steps})')\n",
    "    plt.colorbar(im2, ax=axes[1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Evaluation and visualization completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e8721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone188",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
